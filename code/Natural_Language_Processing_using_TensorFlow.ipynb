{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Natural Language Processing using TensorFlow",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDgaea8ZWYsZ"
      },
      "source": [
        "<h1 align=center><font size = 6>Natural Language Processing using TensorFlow</font></h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmhXIP2fWqWt"
      },
      "source": [
        "Done by\n",
        "Firda Anindita Latifah</p>\n",
        "firdaaninditalatifah@gmail.com</p>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N8hBmfhBpbMm"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nbqfa4suplnS"
      },
      "source": [
        "%cd /content/gdrive/My Drive/Data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kIclSrslp_3I"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('Corona_NLP_train.csv', encoding=\"Latin-1\")\n",
        "df = df[['OriginalTweet','Sentiment']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sfQVJvFOJ69J"
      },
      "source": [
        "for i in range(0,len(df)):\n",
        "    if(df['Sentiment'][i]=='Extremely Negative'):\n",
        "        df['Sentiment'][i]='Negative'\n",
        "    elif(df['Sentiment'][i]=='Extremely Positive'):\n",
        "        df['Sentiment'][i]='Positive'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Drx4avqnrZmK"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "sns.countplot(x=df.Sentiment)\n",
        "plt.title('The number of elements in each class in the data')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJOJHs4op_lB"
      },
      "source": [
        "category = pd.get_dummies(df.Sentiment)\n",
        "df_new = pd.concat([df, category], axis=1)\n",
        "df_new = df_new.drop(columns='Sentiment')\n",
        "df_new"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_G49vojNrKL8"
      },
      "source": [
        "import re\n",
        "from string import punctuation, digits\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "def remove_punctuation(s):\n",
        "    list_punctuation = list(punctuation)\n",
        "    for i in list_punctuation:\n",
        "        s = s.replace(i,'')\n",
        "    return s.lower()\n",
        "\n",
        "def clean_sentence(sentence):\n",
        "    sentence = sentence.lower()\n",
        "    sentence = re.sub(r'(\\W)\\1{2,}', r'\\1', sentence) \n",
        "    sentence = re.sub(r'(\\w)\\1{2,}', r'\\1\\1', sentence)\n",
        "    sentence = re.sub(r'(?P<url>https?://[^\\s]+)', '', sentence)\n",
        "    sentence = re.sub(r\"\\@(\\w+)\", '', sentence) \n",
        "    sentence = re.sub(r\"\\#(\\w+)\", '', sentence) \n",
        "    sentence = re.sub(r\"\\$(\\w+)\", '', sentence) \n",
        "    sentence= re.sub(r'[^\\w]', ' ', sentence)\n",
        "    sentence= re.sub(r\"[&amp]\", '', sentence)\n",
        "    sentence = sentence.replace(\"-\",' ')\n",
        "    tokens = sentence.split()\n",
        "    tokens = [remove_punctuation(w) for w in tokens] \n",
        "    stop_words = set(stopwords.words('english')) \n",
        "    tokens = [w for w in tokens if not w in stop_words]\n",
        "    remove_digits = str.maketrans('', '', digits)\n",
        "    tokens = [w.translate(remove_digits) for w in tokens]\n",
        "    tokens = [w.strip() for w in tokens]\n",
        "    tokens = [w for w in tokens if w!=\"\"]\n",
        "    tokens = ' '.join(tokens)\n",
        "    return tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1DNJG1TMtihi"
      },
      "source": [
        "df_new['OriginalTweet'] = df_new['OriginalTweet'].apply(lambda sentence:clean_sentence(sentence))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D7mSi0PvqP6Y"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud, ImageColorGenerator\n",
        "\n",
        "allWords = ''.join([twts for twts in df_new['OriginalTweet']])\n",
        "wordCloud = WordCloud(width = 300,height =150,random_state = 21,max_font_size =119).generate(allWords)\n",
        "plt.imshow(wordCloud,interpolation = \"bilinear\")\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6uLQIxzLrKGn"
      },
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('wordnet')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "def nltk_tag_to_wordnet_tag(nltk_tag):\n",
        "    if nltk_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif nltk_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif nltk_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif nltk_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "def lemmatize_sentence(de_punct_sent):\n",
        "    nltk_tagged = nltk.pos_tag(nltk.word_tokenize(de_punct_sent))\n",
        "    wordnet_tagged = map(lambda x: (x[0], nltk_tag_to_wordnet_tag(x[1])), nltk_tagged)\n",
        "    lemmatized_sentence = []\n",
        "    for word, tag in wordnet_tagged:\n",
        "        if tag is None:\n",
        "            lemmatized_sentence.append(word)\n",
        "        else:\n",
        "            lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n",
        "    return \" \".join(lemmatized_sentence)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PM8lq07BtReL"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "tokenizer = Tokenizer(num_words=10000, oov_token='x')\n",
        "tokenizer.fit_on_texts(df_baru['OriginalTweet'].values)\n",
        "df_token = tokenizer.texts_to_sequences(df_baru['OriginalTweet'].values)\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvYYxsdByp_z"
      },
      "source": [
        "lens =  [len(s) for s in df_token]\n",
        "\n",
        "plt.title('The maximum number of words in a sentence')\n",
        "plt.xlabel('The max number of words in a sentence')\n",
        "plt.ylabel('The number of sentences')\n",
        "plt.hist(lens,bins=200)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4aqvfB5uyzGG"
      },
      "source": [
        "from tensorflow.keras import preprocessing\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "maxlen = 40\n",
        "df_train = pad_sequences(df_token, maxlen=maxlen, padding='post', truncating='post') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqTG9cmBzGhv"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(input_dim=25000, output_dim=16),\n",
        "    tf.keras.layers.LSTM(128, dropout=0.2, recurrent_dropout=0.2),\n",
        "    tf.keras.layers.Dense(256, activation='relu'),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(3, activation='softmax')\n",
        "])\n",
        "model.compile(loss='categorical_crossentropy', optimizer=keras.optimizers.Adam(learning_rate=0.001), metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1rJ0oNQwzGcv"
      },
      "source": [
        "class Call(tf.keras.callbacks.Callback): \n",
        "  def on_epoch_end(self, epoch, logs={}): \n",
        "    if(logs.get('accuracy') > 0.85 and logs.get('val_accuracy') > 0.85):\n",
        "      print(\"\\nAccuracy > 85%\") \n",
        "      self.model.stop_training = True \n",
        " \n",
        "callbacks = Call()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ruQAwGi6z2Ze"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "label = df_new[['Negative',\t'Neutral',\t'Positive']].values\n",
        "train_data, test_data, train_label, test_label = train_test_split(df_train, label, test_size = 0.2, random_state = 42)\n",
        "print(train_data.shape,train_label.shape)\n",
        "print(test_data.shape,test_label.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ff1OMewzGXV"
      },
      "source": [
        "history = model.fit(train_data, train_label, epochs=10, \n",
        "                    validation_data =(test_data, test_label), verbose=2, callbacks=[callbacks])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EyvGLhg86Mur"
      },
      "source": [
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy'] \n",
        "loss = history.history['loss'] \n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "plt.plot(epochs, acc, 'r', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc') \n",
        "plt.title('Training and validation accuracy') \n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'r', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss') \n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}